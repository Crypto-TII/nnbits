# DATA INFORMATION 
# The data should be contained in a single *.npy file stored in `datapath`.
# Note: When loaded by using `data=np.load(datapath)`, the `data.shape` is expected to be 
# (number of sequences, number of bits per sequence).
datapath = '/opt/cryptanalysis_servers_shared_folder/NBEATSCipherDistinguisher/D5/speck_32_64/round4_sequences200k_.npy'
# The number of bits per sequence is 32**2 = 1024 for Speck 32; 128**2=16384 for Speck 128. It is stored in `N_BITS`:
N_BITS = 1024

# HARDWARE INFORMATION 
# How many GPUs will be used:
N_GPUS = 4 
# Each GPU will host a certain number of actors. 
# Note: This number will depend on the model and data complexity. 
# We have used N_ACTORS_PER_GPU = 3 for Speck 32 and N_ACTORS_PER_GPU=2 for Speck 128.
N_ACTORS_PER_GPU = 6 
# GPU fraction per actor (typically 0.33 for Speck 32 and 0.5 for Speck 128 for GPUs with 40GB memory per GPU):
NUM_GPUS=0.15
# CPUs per actor (typically 8 for Speck 32 and 28 for Speck 128 if you have 128 CPU cores available):
NUM_CPUS=5

# MODEL INFORMATION 
# The `MODEL_ID` identifies which model is to be used. 
# All models defined in a Python file *.py in folder `models` and imported as `MODEL_ID` in __init__.py can be used.
MODEL_ID = 'nbeats'
# Some models like MLP can be scaled to a higher representational power. 
# The scaling effect is defined in the respective Python file in `models` 
MODEL_STRENGTH = 1 
# Define the number of neurons present at the model input and output:
MODEL_INPUTS = 961
MODEL_OUTPUTS = 63
# Should the model weights be saved? 
SAVE_WEIGHTS = false 

# TRAINING INFORMATION
# For how many epochs should the model be trained?
N_EPOCHS = 10
# How many sequences to use for training 
# (the batch size is set to 4096, so please choose a multiple of 4096):
N_TRAIN = 4096 
# How many sequences to use for validation:
N_VAL = 4096 
# Batchsize for training
BATCHSIZE = 4096
# Early stopping of the training if the p-value is below 1e-10 (EARLY_STOPPING_P_VALUE in run.py)
EARLY_STOPPING = true

# DATA INFORMATION
# You can either make new filters or choose to use pre-existing filters in the savepath
MAKE_NEW_FILTERS = false
# Various randomly selected combinations of inputs and output bits will be tried out.
# How many combinations are tried is given by the number of filters `N_FILTERS`.
N_FILTERS = 50
# How many bit ids should be selected as input bits?
# For Speck 32 choose 31**2 = 961 as N_INPUTS
# For Speck 128 choose 126**2 = 15876
N_INPUT_FILTER_ELEMENTS = 961
# For the creation of filters, different strategies can be chosen
# 'random': choose the predicted bits at random, however, across `N_FILTERS` 
#           we try to assure that each bit is chosen the approximate same number of times
# 'target': if the strategy is targetted, a list of target bits corresponding to N_FILTERS has to be provided.
# 'gohr_with_target': The target bit will be zero-ed out and the predicted bit is the last one in the dataset. 
#            Use this filter strategy together with a DATA_STRATEGY='zero_gohr' and N_INPUTS=64 and N_BITS=65.
FILTER_STRATEGY = 'random' 
TARGET_BITS = []
# The bits which are selected as output bits have to be removed from the input in some way. 
# 'zero': In this strategy the bits will only be zero-ed at the input to remove their information. This will preserve the data shape.
# 'remove': In this strategy the bits will actually be removed from the input data.
# 'zero_gohr': The target bit will be zero-ed out and the predicted bit is the last one in the dataset provided. 
#            Use together with filter strategy 'gohr_with_target'. 
DATA_STRATEGY = 'remove'