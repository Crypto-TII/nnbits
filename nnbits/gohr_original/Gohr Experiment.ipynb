{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "052c9b7e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-16T05:17:09.705069Z",
     "start_time": "2022-04-16T05:17:08.294130Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from gohr import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a67b42b3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-16T05:17:17.948362Z",
     "start_time": "2022-04-16T05:17:17.943380Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def train_speck_distinguisher(num_epochs, nr, n, depth=1):\n",
    "    #create the network\n",
    "    net = make_resnet(depth=depth, reg_param=10**-5);\n",
    "    net.compile(optimizer='adam',loss='mse',metrics=['acc']);\n",
    "    #generate training and validation data\n",
    "    X, Y = make_train_data(n, nr);\n",
    "    X_eval, Y_eval = make_train_data(n//10, nr);\n",
    "    #create learnrate schedule\n",
    "    lr = LearningRateScheduler(cyclic_lr(10,0.002, 0.0001));\n",
    "    #train and evaluate\n",
    "    h = net.fit(X,Y,epochs=num_epochs,batch_size=bs,validation_data=(X_eval, Y_eval), callbacks=[lr]);\n",
    "\n",
    "    np.save('data_train_full.npy', X);\n",
    "    np.save('data_train_labels.npy', Y);\n",
    "    np.save('data_train_real_samples.npy', X[np.where(Y==1)]);\n",
    "\n",
    "    np.save('data_test_full.npy', X_eval);\n",
    "    np.save('data_test_labels.npy', Y_eval);\n",
    "    np.save('data_test_real_samples.npy', X_eval[np.where(Y_eval==1)]);\n",
    "\n",
    "    print(\"Best validation accuracy: \", np.max(h.history['val_acc']));\n",
    "\n",
    "    return(X, Y);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a8f662f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-16T05:37:07.630554Z",
     "start_time": "2022-04-16T05:37:07.326755Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/cryptanalysis_servers_shared_folder/NBEATSCipherDistinguisher/nbeats_statistical_test/gohr\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb8e83cc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-16T05:19:41.535095Z",
     "start_time": "2022-04-16T05:17:24.274939Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-16 09:17:24.935646: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-04-16 09:17:29.077469: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38414 MB memory:  -> device: 0, name: A100-SXM4-40GB, pci bus id: 0000:01:00.0, compute capability: 8.0\n",
      "2022-04-16 09:17:29.079485: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 38414 MB memory:  -> device: 1, name: A100-SXM4-40GB, pci bus id: 0000:47:00.0, compute capability: 8.0\n",
      "2022-04-16 09:17:29.081349: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 38414 MB memory:  -> device: 2, name: A100-SXM4-40GB, pci bus id: 0000:81:00.0, compute capability: 8.0\n",
      "2022-04-16 09:17:29.083240: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:3 with 38414 MB memory:  -> device: 3, name: A100-SXM4-40GB, pci bus id: 0000:c2:00.0, compute capability: 8.0\n",
      "2022-04-16 09:17:29.084145: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:4 with 2564 MB memory:  -> device: 4, name: DGX Display, pci bus id: 0000:c1:00.0, compute capability: 7.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-16 09:17:38.643606: I tensorflow/stream_executor/cuda/cuda_dnn.cc:366] Loaded cuDNN version 8200\n",
      "2022-04-16 09:17:40.054543: W tensorflow/stream_executor/gpu/asm_compiler.cc:111] *** WARNING *** You are using ptxas 11.0.221, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "You may not need to update to CUDA 11.1; cherry-picking the ptxas binary is often sufficient.\n",
      "2022-04-16 09:17:41.294080: I tensorflow/stream_executor/cuda/cuda_blas.cc:1774] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/2000 [==============================] - 18s 6ms/step - loss: 0.1794 - acc: 0.7312 - val_loss: 0.1626 - val_acc: 0.7651 - lr: 0.0020\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 11s 5ms/step - loss: 0.1565 - acc: 0.7748 - val_loss: 0.1554 - val_acc: 0.7769 - lr: 0.0018\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 11s 6ms/step - loss: 0.1542 - acc: 0.7785 - val_loss: 0.1543 - val_acc: 0.7785 - lr: 0.0016\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 11s 6ms/step - loss: 0.1532 - acc: 0.7798 - val_loss: 0.1537 - val_acc: 0.7797 - lr: 0.0014\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 10s 5ms/step - loss: 0.1525 - acc: 0.7806 - val_loss: 0.1527 - val_acc: 0.7803 - lr: 0.0012\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 11s 5ms/step - loss: 0.1519 - acc: 0.7814 - val_loss: 0.1522 - val_acc: 0.7808 - lr: 9.4444e-04\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 11s 5ms/step - loss: 0.1513 - acc: 0.7821 - val_loss: 0.1517 - val_acc: 0.7816 - lr: 7.3333e-04\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 11s 6ms/step - loss: 0.1507 - acc: 0.7828 - val_loss: 0.1511 - val_acc: 0.7824 - lr: 5.2222e-04\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 10s 5ms/step - loss: 0.1502 - acc: 0.7836 - val_loss: 0.1507 - val_acc: 0.7828 - lr: 3.1111e-04\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 11s 5ms/step - loss: 0.1496 - acc: 0.7844 - val_loss: 0.1504 - val_acc: 0.7833 - lr: 1.0000e-04\n",
      "Best validation accuracy:  0.783257007598877\n"
     ]
    }
   ],
   "source": [
    "X, Y = train_speck_distinguisher(10, 6, 10**7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f45d52",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# `generalized_gohr` "
   ]
  },
  {
   "cell_type": "raw",
   "id": "fbf14bce",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T13:27:32.664164Z",
     "start_time": "2022-04-19T13:27:32.653053Z"
    },
    "pycharm": {
     "name": "#%% raw\n"
    }
   },
   "source": [
    "def create_gohr_generalized_model(input_neurons=32, output_neurons=10, model_strength=1, set_memory_growth=True):\n",
    "    \"\"\"\n",
    "    Epoch 1/10\n",
    "    2000/2000 [==============================] - 31s 12ms/step - loss: 0.1970 - acc: 0.7036 - val_loss: 0.1756 - val_acc: 0.7428\n",
    "    Epoch 2/10\n",
    "    2000/2000 [==============================] - 24s 12ms/step - loss: 0.1655 - acc: 0.7587 - val_loss: 0.1622 - val_acc: 0.7645\n",
    "    Epoch 3/10\n",
    "    2000/2000 [==============================] - 23s 12ms/step - loss: 0.1572 - acc: 0.7733 - val_loss: 0.1561 - val_acc: 0.7755\n",
    "    Epoch 4/10\n",
    "    2000/2000 [==============================] - 24s 12ms/step - loss: 0.1549 - acc: 0.7769 - val_loss: 0.1550 - val_acc: 0.7771\n",
    "    Epoch 5/10\n",
    "    2000/2000 [==============================] - 24s 12ms/step - loss: 0.1539 - acc: 0.7783 - val_loss: 0.1539 - val_acc: 0.7789\n",
    "    Epoch 6/10\n",
    "    2000/2000 [==============================] - 24s 12ms/step - loss: 0.1531 - acc: 0.7800 - val_loss: 0.1531 - val_acc: 0.7802\n",
    "    Epoch 7/10\n",
    "    2000/2000 [==============================] - 24s 12ms/step - loss: 0.1526 - acc: 0.7807 - val_loss: 0.1527 - val_acc: 0.7807\n",
    "    Epoch 8/10\n",
    "    2000/2000 [==============================] - 24s 12ms/step - loss: 0.1522 - acc: 0.7811 - val_loss: 0.1525 - val_acc: 0.7808\n",
    "    Epoch 9/10\n",
    "    2000/2000 [==============================] - 24s 12ms/step - loss: 0.1520 - acc: 0.7815 - val_loss: 0.1522 - val_acc: 0.7811\n",
    "    Epoch 10/10\n",
    "    2000/2000 [==============================] - 24s 12ms/step - loss: 0.1517 - acc: 0.7818 - val_loss: 0.1521 - val_acc: 0.7814\n",
    "    Best validation accuracy:  0.781387984752655\n",
    "    \"\"\"\n",
    "    #--- prepare GPU\n",
    "    import tensorflow as tf\n",
    "    gpus = tf.config.experimental.list_physical_devices(\"GPU\")\n",
    "    tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
    "    if set_memory_growth:\n",
    "        tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "        \n",
    "    \n",
    "\n",
    "    from keras.models import Model\n",
    "    from keras.layers import Dense, Conv1D, Conv2D, Input, Reshape, Permute, Add, Flatten, BatchNormalization, Activation\n",
    "    from keras.layers import Concatenate, MaxPooling2D\n",
    "    from keras.regularizers import l2\n",
    "    import numpy as np\n",
    "    \n",
    "    img_sqrt = int(np.sqrt(input_neurons))\n",
    "    \n",
    "    #num_blocks=2 \n",
    "    num_filters=32*4\n",
    "    d1=64*model_strength\n",
    "    d2=64*model_strength\n",
    "    #word_size=16\n",
    "    ks=3\n",
    "    depth=model_strength\n",
    "    reg_param=10**-5       \n",
    "    final_activation='sigmoid'\n",
    "    \n",
    "    optimizer = 'Adam'\n",
    "    loss = 'mse' #'Huber'\n",
    "    \n",
    "    #loss = 'mse'\n",
    "\n",
    "    # put input in square shape instead of word-like structure\n",
    "    inp = Input(shape=(input_neurons,));\n",
    "    #rs = Reshape((2 * num_blocks, word_size))(inp);\n",
    "    rs = Reshape((img_sqrt, img_sqrt))(inp) # changed rs = Reshape((img_sqrt, img_sqrt, 1))(inp)\n",
    "    \n",
    "    #---- search correlations in one direction\n",
    "    conv0 = Conv1D(num_filters, kernel_size=1, padding='same', kernel_regularizer=l2(reg_param))(rs); # changed\n",
    "    conv0 = BatchNormalization()(conv0);\n",
    "    conv0 = Activation('relu')(conv0);\n",
    "    #add residual blocks \n",
    "    shortcut = conv0;\n",
    "    for i in range(depth):\n",
    "        conv1 = Conv1D(num_filters, kernel_size=ks, padding='same', kernel_regularizer=l2(reg_param))(shortcut);\n",
    "        conv1 = BatchNormalization()(conv1);\n",
    "        conv1 = Activation('relu')(conv1);\n",
    "        conv2 = Conv1D(num_filters, kernel_size=ks, padding='same',kernel_regularizer=l2(reg_param))(conv1);\n",
    "        conv2 = BatchNormalization()(conv2);\n",
    "        conv2 = Activation('relu')(conv2);\n",
    "        shortcut = Add()([shortcut, conv2]);\n",
    "        \n",
    "    #---- search correlations in the other direction\n",
    "    perm = Permute((2,1))(rs); \n",
    "    conv0 = Conv1D(num_filters, kernel_size=1, padding='same', kernel_regularizer=l2(reg_param))(perm); # changed\n",
    "    conv0 = BatchNormalization()(conv0);\n",
    "    conv0 = Activation('relu')(conv0);\n",
    "    shortcut1 = conv0;\n",
    "    for i in range(depth):\n",
    "        conv1 = Conv1D(num_filters, kernel_size=ks, padding='same', kernel_regularizer=l2(reg_param))(shortcut1);\n",
    "        conv1 = BatchNormalization()(conv1);\n",
    "        conv1 = Activation('relu')(conv1);\n",
    "        conv2 = Conv1D(num_filters, kernel_size=ks, padding='same',kernel_regularizer=l2(reg_param))(conv1);\n",
    "        conv2 = BatchNormalization()(conv2);\n",
    "        conv2 = Activation('relu')(conv2);\n",
    "        shortcut1 = Add()([shortcut1, conv2]);\n",
    "\n",
    "    #---- bring them together\n",
    "    shortcut = Concatenate()([shortcut, shortcut1])#Add()([shortcut, shortcut1])\n",
    "    #add prediction head\n",
    "    flat1 = Flatten()(shortcut);\n",
    "    dense1 = Dense(d1,kernel_regularizer=l2(reg_param))(flat1);\n",
    "    dense1 = BatchNormalization()(dense1);\n",
    "    dense1 = Activation('relu')(dense1);\n",
    "    dense2 = Dense(d2, kernel_regularizer=l2(reg_param))(dense1);\n",
    "    dense2 = BatchNormalization()(dense2);\n",
    "    dense2 = Activation('relu')(dense2);\n",
    "    out = Dense(output_neurons, activation=final_activation, kernel_regularizer=l2(reg_param))(dense2);\n",
    "    model = Model(inputs=inp, outputs=out);\n",
    "    #-------------#-------------#-------------#-------------#\n",
    "    \n",
    "    model.compile(loss=loss,\n",
    "                  optimizer=optimizer,\n",
    "                  run_eagerly=False, metrics=['acc'])  \n",
    "        \n",
    "    from gohr import cyclic_lr\n",
    "    from keras.callbacks import LearningRateScheduler\n",
    "    lr = LearningRateScheduler(cyclic_lr(10,0.002, 0.0001));\n",
    "    \n",
    "    model.callbacks = [lr]\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d526d29",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-20T12:03:41.687615Z",
     "start_time": "2022-04-20T12:03:40.381148Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Conv1D, Conv2D, Input, Reshape, Permute, Add, Flatten, BatchNormalization, Activation\n",
    "from keras.layers import Concatenate, MaxPooling2D\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3549d0c9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-20T12:05:32.620500Z",
     "start_time": "2022-04-20T12:05:30.419742Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " reshape_2 (Reshape)         (None, 4, 16)             0         \n",
      "                                                                 \n",
      " permute_2 (Permute)         (None, 16, 4)             0         \n",
      "                                                                 \n",
      " conv1d_2 (Conv1D)           (None, 16, 64)            320       \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 16, 64)            0         \n",
      "                                                                 \n",
      " conv1d_3 (Conv1D)           (None, 16, 32)            6176      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 6,496\n",
      "Trainable params: 6,496\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-20 16:05:31.366320: I tensorflow/stream_executor/cuda/cuda_dnn.cc:366] Loaded cuDNN version 8200\n",
      "2022-04-20 16:05:32.568112: W tensorflow/stream_executor/gpu/asm_compiler.cc:111] *** WARNING *** You are using ptxas 11.0.221, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "You may not need to update to CUDA 11.1; cherry-picking the ptxas binary is often sufficient.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 16, 32), dtype=float32, numpy=\n",
       "array([[[ -2.6095324 ,   3.604008  ,   7.9089074 ,   0.41484582,\n",
       "          -2.2607586 ,   0.96337605,   3.2733598 ,   3.1761317 ,\n",
       "          -7.1126733 ,  -0.6546638 ,  -5.285961  ,   4.1520452 ,\n",
       "           6.718466  ,   1.1615256 ,  -4.787424  ,  -5.5447054 ,\n",
       "           2.642722  ,  11.261836  ,   5.363558  ,   7.823221  ,\n",
       "         -10.500961  ,   4.74484   ,   8.375633  ,   7.7465806 ,\n",
       "         -11.2287655 ,  12.0367565 ,   7.1719213 ,   0.18827105,\n",
       "          -3.2608054 ,  -2.3985646 ,   6.828616  ,   3.0558486 ],\n",
       "        [  4.287055  ,  -0.31594914,   4.2864065 ,   2.746269  ,\n",
       "          -4.1474657 ,   4.1373544 ,  12.101219  ,   1.2856698 ,\n",
       "          -9.978552  ,  -5.5343833 ,  -8.619227  ,   2.4149196 ,\n",
       "           4.7569103 ,   0.18530636,   2.087522  , -10.924616  ,\n",
       "          -0.726223  ,   8.565805  ,   7.7244377 ,   8.420779  ,\n",
       "         -12.647728  ,  10.831197  ,   1.9039855 ,   5.440014  ,\n",
       "         -14.281349  ,  16.78512   ,   1.5878575 ,  -4.2680907 ,\n",
       "           1.3313277 ,  -8.186262  ,   2.473421  ,   2.436543  ],\n",
       "        [  4.421518  ,  -0.6328089 ,   4.499672  ,   2.8785448 ,\n",
       "          -4.0299134 ,   4.4184833 ,  12.240641  ,   1.3474896 ,\n",
       "         -10.271341  ,  -5.622586  ,  -8.557471  ,   2.4121008 ,\n",
       "           4.848136  ,   0.14512287,   2.3041265 , -10.949964  ,\n",
       "          -0.8072574 ,   8.73792   ,   7.9250226 ,   8.888105  ,\n",
       "         -13.389325  ,  11.08271   ,   2.0522616 ,   5.4605303 ,\n",
       "         -14.369746  ,  17.17131   ,   1.5597472 ,  -4.131795  ,\n",
       "           1.6882479 ,  -8.76347   ,   2.594641  ,   2.2691698 ],\n",
       "        [  4.5261893 ,  -0.92175233,   4.6765366 ,   3.016649  ,\n",
       "          -3.884758  ,   4.7088594 ,  12.403631  ,   1.446468  ,\n",
       "         -10.590895  ,  -5.741685  ,  -8.4860935 ,   2.3917708 ,\n",
       "           4.9862933 ,   0.11634651,   2.478631  , -10.9430485 ,\n",
       "          -0.88674533,   8.926552  ,   8.123856  ,   9.337644  ,\n",
       "         -14.103801  ,  11.3253765 ,   2.2259467 ,   5.441524  ,\n",
       "         -14.460945  ,  17.579618  ,   1.5646203 ,  -3.9908605 ,\n",
       "           2.009305  ,  -9.3348875 ,   2.6848629 ,   2.1313267 ],\n",
       "        [  4.6202145 ,  -1.201551  ,   4.8405123 ,   3.1647096 ,\n",
       "          -3.72779   ,   4.9961376 ,  12.583532  ,   1.5583699 ,\n",
       "         -10.916582  ,  -5.878372  ,  -8.4185    ,   2.3653386 ,\n",
       "           5.140292  ,   0.10017089,   2.6376534 , -10.925121  ,\n",
       "          -0.9721596 ,   9.130051  ,   8.318573  ,   9.780443  ,\n",
       "         -14.80587   ,  11.563646  ,   2.4117765 ,   5.4094706 ,\n",
       "         -14.547032  ,  17.996382  ,   1.5842283 ,  -3.854251  ,\n",
       "           2.3138585 ,  -9.907094  ,   2.7600043 ,   2.000701  ],\n",
       "        [  4.714241  ,  -1.4813504 ,   5.004489  ,   3.3127694 ,\n",
       "          -3.5708232 ,   5.283416  ,  12.763433  ,   1.670271  ,\n",
       "         -11.24227   ,  -6.01506   ,  -8.350904  ,   2.3389063 ,\n",
       "           5.2942905 ,   0.08399558,   2.7966752 , -10.907192  ,\n",
       "          -1.0575738 ,   9.333549  ,   8.5132885 ,  10.223242  ,\n",
       "         -15.507936  ,  11.80192   ,   2.5976057 ,   5.3774157 ,\n",
       "         -14.633118  ,  18.413147  ,   1.6038368 ,  -3.717642  ,\n",
       "           2.618414  , -10.4793    ,   2.8351457 ,   1.8700728 ],\n",
       "        [  4.808265  ,  -1.761149  ,   5.1684647 ,   3.4608285 ,\n",
       "          -3.413857  ,   5.570694  ,  12.943333  ,   1.7821712 ,\n",
       "         -11.567959  ,  -6.1517477 ,  -8.283312  ,   2.312474  ,\n",
       "           5.448288  ,   0.06782089,   2.955696  , -10.889265  ,\n",
       "          -1.1429864 ,   9.537047  ,   8.708006  ,  10.66604   ,\n",
       "         -16.210007  ,  12.040194  ,   2.7834349 ,   5.3453627 ,\n",
       "         -14.719206  ,  18.82991   ,   1.6234436 ,  -3.5810325 ,\n",
       "           2.922967  , -11.051506  ,   2.910287  ,   1.7394458 ],\n",
       "        [  4.915538  ,  -2.044888  ,   5.31066   ,   3.6162832 ,\n",
       "          -3.2339137 ,   5.870486  ,  13.138027  ,   1.9122814 ,\n",
       "         -11.900866  ,  -6.271251  ,  -8.211348  ,   2.3086662 ,\n",
       "           5.580552  ,   0.05476714,   3.1144834 , -10.893742  ,\n",
       "          -1.2240933 ,   9.731199  ,   8.896145  ,  11.091244  ,\n",
       "         -16.911007  ,  12.261383  ,   2.9685192 ,   5.2922635 ,\n",
       "         -14.806115  ,  19.226778  ,   1.6446788 ,  -3.4447708 ,\n",
       "           3.2315197 , -11.608441  ,   2.9774108 ,   1.6215707 ],\n",
       "        [  5.0373597 ,  -2.3206792 ,   5.4340677 ,   3.7623887 ,\n",
       "          -3.0607471 ,   6.1836867 ,  13.347506  ,   2.068747  ,\n",
       "         -12.222591  ,  -6.3781185 ,  -8.132875  ,   2.3082232 ,\n",
       "           5.699544  ,   0.05756168,   3.2611585 , -10.919762  ,\n",
       "          -1.2939508 ,   9.91077   ,   9.07394   ,  11.529509  ,\n",
       "         -17.61821   ,  12.498455  ,   3.1579423 ,   5.2225394 ,\n",
       "         -14.875726  ,  19.61238   ,   1.6547995 ,  -3.3178315 ,\n",
       "           3.523415  , -12.148417  ,   3.0518117 ,   1.4915804 ],\n",
       "        [  5.169998  ,  -2.6045675 ,   5.5571046 ,   3.9124756 ,\n",
       "          -2.909634  ,   6.4980354 ,  13.570807  ,   2.221317  ,\n",
       "         -12.513842  ,  -6.4683237 ,  -8.040325  ,   2.3019238 ,\n",
       "           5.8110476 ,   0.08428923,   3.408914  , -10.9427595 ,\n",
       "          -1.3802272 ,  10.064228  ,   9.250998  ,  11.996139  ,\n",
       "         -18.319706  ,  12.745392  ,   3.3716269 ,   5.1442704 ,\n",
       "         -14.926278  ,  20.019249  ,   1.6517273 ,  -3.1904309 ,\n",
       "           3.7929125 , -12.6706295 ,   3.1471372 ,   1.347153  ],\n",
       "        [  5.310597  ,  -2.8973095 ,   5.68978   ,   4.06694   ,\n",
       "          -2.7629473 ,   6.825016  ,  13.7987385 ,   2.3608658 ,\n",
       "         -12.792074  ,  -6.5627346 ,  -7.951043  ,   2.2869322 ,\n",
       "           5.9164953 ,   0.12537065,   3.5639505 , -10.958662  ,\n",
       "          -1.471626  ,  10.208713  ,   9.436601  ,  12.476135  ,\n",
       "         -19.011171  ,  12.995489  ,   3.6016803 ,   5.0585804 ,\n",
       "         -14.977402  ,  20.44727   ,   1.6511567 ,  -3.0758128 ,\n",
       "           4.0628457 , -13.190322  ,   3.244447  ,   1.1986659 ],\n",
       "        [  5.447832  ,  -3.1743956 ,   5.8320837 ,   4.233509  ,\n",
       "          -2.606575  ,   7.161579  ,  14.027559  ,   2.496867  ,\n",
       "         -13.0662155 ,  -6.6544213 ,  -7.874053  ,   2.2666228 ,\n",
       "           6.0246215 ,   0.17526671,   3.716398  , -10.969086  ,\n",
       "          -1.5593019 ,  10.3418255 ,   9.621402  ,  12.969385  ,\n",
       "         -19.70874   ,  13.24071   ,   3.81523   ,   4.9601865 ,\n",
       "         -15.046007  ,  20.885263  ,   1.6498361 ,  -2.9706182 ,\n",
       "           4.3350306 , -13.711119  ,   3.3475602 ,   1.0523264 ],\n",
       "        [  5.5808144 ,  -3.438656  ,   5.9830847 ,   4.4102306 ,\n",
       "          -2.450434  ,   7.4959335 ,  14.268768  ,   2.6351066 ,\n",
       "         -13.352453  ,  -6.743124  ,  -7.8076916 ,   2.2337852 ,\n",
       "           6.141923  ,   0.22061315,   3.8589532 , -10.986238  ,\n",
       "          -1.6558666 ,  10.480071  ,   9.818967  ,  13.472909  ,\n",
       "         -20.4052    ,  13.496871  ,   4.0167494 ,   4.8600006 ,\n",
       "         -15.12878   ,  21.319387  ,   1.6345993 ,  -2.8513079 ,\n",
       "           4.595435  , -14.216757  ,   3.4562914 ,   0.89742446],\n",
       "        [  5.7137957 ,  -3.7029176 ,   6.1340857 ,   4.586952  ,\n",
       "          -2.2942955 ,   7.830285  ,  14.509981  ,   2.7733433 ,\n",
       "         -13.638687  ,  -6.8318224 ,  -7.741327  ,   2.2009478 ,\n",
       "           6.2592278 ,   0.26595926,   4.0015106 , -11.003387  ,\n",
       "          -1.7524312 ,  10.618317  ,  10.016533  ,  13.976435  ,\n",
       "         -21.10166   ,  13.753035  ,   4.2182684 ,   4.759815  ,\n",
       "         -15.21155   ,  21.753513  ,   1.6193645 ,  -2.7319965 ,\n",
       "           4.8558397 , -14.722393  ,   3.565024  ,   0.7425223 ],\n",
       "        [  5.861864  ,  -3.9535804 ,   6.2885294 ,   4.7617807 ,\n",
       "          -2.136437  ,   8.167236  ,  14.746627  ,   2.9219317 ,\n",
       "         -13.913357  ,  -6.934024  ,  -7.662545  ,   2.1761036 ,\n",
       "           6.3715773 ,   0.30744773,   4.139979  , -11.033148  ,\n",
       "          -1.8579919 ,  10.772526  ,  10.217899  ,  14.466682  ,\n",
       "         -21.812056  ,  14.00602   ,   4.406638  ,   4.656205  ,\n",
       "         -15.30481   ,  22.178295  ,   1.5987993 ,  -2.6249776 ,\n",
       "           5.121721  , -15.239162  ,   3.6851468 ,   0.5998894 ],\n",
       "        [  3.2291522 ,  -9.670685  ,   1.4536469 ,   3.749087  ,\n",
       "          -4.8465834 ,   5.278877  ,  18.143093  ,  -7.7054477 ,\n",
       "         -10.820588  ,   0.671916  ,  -1.8823402 ,   7.1391134 ,\n",
       "           9.857891  ,   1.8466595 ,  13.929378  ,  -5.733415  ,\n",
       "           1.8148886 ,   5.77471   ,   6.9427614 ,  11.97121   ,\n",
       "         -13.041408  ,  21.89154   ,  -1.7779851 ,   1.1883297 ,\n",
       "          -7.8581266 ,  14.582018  ,   3.1205914 ,   4.1566315 ,\n",
       "           8.3636675 ,  -7.370881  ,   5.7555895 ,  -3.034555  ]]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_neurons = 64\n",
    "num_filters = 32\n",
    "\n",
    "import numpy as np\n",
    "img_sqrt = int(np.sqrt(input_neurons))\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.layers import Dense, Conv1D, Conv2D, Input, Reshape, Permute, Add, Flatten, BatchNormalization, Activation\n",
    "from keras.layers import Concatenate, MaxPooling2D\n",
    "\n",
    "model = keras.Sequential()\n",
    "model.add(Input(shape=(input_neurons)))\n",
    "model.add(Reshape((4,16))) \n",
    "model.add(Permute((2,1)))\n",
    "model.add(Conv1D(num_filters*2, kernel_size=1, padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv1D(num_filters, kernel_size=3, padding='same'))\n",
    "model.summary()\n",
    "\n",
    "# model = keras.Sequential()\n",
    "# model.add(Input(shape=(input_neurons)))\n",
    "# model.add(Reshape((img_sqrt,img_sqrt)))\n",
    "# model.add(Permute((2,1)))\n",
    "# #model.add(Conv1D(num_filters*2, kernel_size=1, padding='same'))\n",
    "# #model.add(Conv1D(num_filters, kernel_size=3, padding='same'))\n",
    "# model.summary()\n",
    "\n",
    "# Call model on a test input\n",
    "x = tf.convert_to_tensor(np.arange(input_neurons))\n",
    "x = tf.reshape(x, (1, input_neurons)) # to make x a valid model input, the first dimension has to be the number of samples (1)\n",
    "y = model(x)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90c62a81",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-20T12:05:32.626005Z",
     "start_time": "2022-04-20T12:05:32.621759Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'conv1d_3/kernel:0' shape=(3, 64, 32) dtype=float32, numpy=\n",
       " array([[[ 0.14354047,  0.02458915, -0.06277097, ..., -0.10283241,\n",
       "           0.12815365, -0.12849376],\n",
       "         [-0.10094772, -0.02960481, -0.01359944, ..., -0.06570765,\n",
       "          -0.09763581,  0.02405043],\n",
       "         [ 0.11872768, -0.08624876,  0.09596403, ..., -0.05429596,\n",
       "          -0.10883072,  0.09438436],\n",
       "         ...,\n",
       "         [ 0.02743533,  0.11702055, -0.13368183, ...,  0.00112206,\n",
       "          -0.08765683,  0.12213826],\n",
       "         [ 0.14402303,  0.02894667, -0.12337376, ...,  0.12333024,\n",
       "           0.09687178, -0.10157792],\n",
       "         [-0.14417046,  0.12276313, -0.03695438, ...,  0.03988273,\n",
       "           0.11471289,  0.03740819]],\n",
       " \n",
       "        [[ 0.11059678, -0.122151  ,  0.09708235, ...,  0.07853787,\n",
       "          -0.10125114, -0.02636334],\n",
       "         [-0.11923363, -0.11846702,  0.0266353 , ..., -0.04778473,\n",
       "           0.12043029, -0.10257222],\n",
       "         [ 0.13244271,  0.08256088, -0.06939463, ...,  0.01165184,\n",
       "          -0.10336202,  0.07252413],\n",
       "         ...,\n",
       "         [-0.06773542,  0.06046228,  0.09721336, ..., -0.01892269,\n",
       "           0.04255836,  0.13250887],\n",
       "         [-0.08942771,  0.12750939,  0.10972267, ...,  0.06183001,\n",
       "          -0.08647186, -0.11226959],\n",
       "         [-0.12319113,  0.10672683,  0.04921761, ...,  0.06344174,\n",
       "          -0.05293814, -0.13698463]],\n",
       " \n",
       "        [[-0.05880269,  0.09556492,  0.12812757, ...,  0.00305565,\n",
       "          -0.03836696,  0.04547784],\n",
       "         [ 0.07242897, -0.09761818,  0.1022573 , ..., -0.08252527,\n",
       "           0.05431066, -0.01525876],\n",
       "         [-0.01256169,  0.04065795,  0.02867849, ..., -0.13923621,\n",
       "           0.1386998 , -0.04650874],\n",
       "         ...,\n",
       "         [-0.01449572,  0.0917706 , -0.0771843 , ...,  0.05163015,\n",
       "          -0.08856973, -0.04837673],\n",
       "         [ 0.03098825, -0.12265722, -0.02008305, ...,  0.06738858,\n",
       "           0.1262699 ,  0.12292692],\n",
       "         [-0.10016927, -0.05954085, -0.08294648, ...,  0.01341984,\n",
       "          -0.12312324, -0.01563348]]], dtype=float32)>,\n",
       " <tf.Variable 'conv1d_3/bias:0' shape=(32,) dtype=float32, numpy=\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       dtype=float32)>]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[-1].weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7d1e1478",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-20T11:32:30.014408Z",
     "start_time": "2022-04-20T11:32:30.009356Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(8, 8), dtype=int64, numpy=\n",
       "array([[ 0,  1,  2,  3,  4,  5,  6,  7],\n",
       "       [ 8,  9, 10, 11, 12, 13, 14, 15],\n",
       "       [16, 17, 18, 19, 20, 21, 22, 23],\n",
       "       [24, 25, 26, 27, 28, 29, 30, 31],\n",
       "       [32, 33, 34, 35, 36, 37, 38, 39],\n",
       "       [40, 41, 42, 43, 44, 45, 46, 47],\n",
       "       [48, 49, 50, 51, 52, 53, 54, 55],\n",
       "       [56, 57, 58, 59, 60, 61, 62, 63]])>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_neurons = 64\n",
    "x = tf.convert_to_tensor(np.arange(input_neurons))\n",
    "x = tf.reshape(x, (8,8))\n",
    "x"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3d7da735",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-20T11:32:20.207511Z",
     "start_time": "2022-04-20T11:32:20.199548Z"
    },
    "pycharm": {
     "name": "#%% raw\n"
    }
   },
   "source": [
    "tf.roll(x, 1, 1)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c79185d4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-20T06:39:47.477932Z",
     "start_time": "2022-04-20T06:39:47.471383Z"
    },
    "pycharm": {
     "name": "#%% raw\n"
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d454cca7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-20T11:32:15.582282Z",
     "start_time": "2022-04-20T11:32:15.580152Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9fa83d1a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-20T10:48:26.570630Z",
     "start_time": "2022-04-20T10:48:26.558633Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def create_gohr_generalized_model(input_neurons=32, output_neurons=10, model_strength=1, set_memory_growth=True):\n",
    "    \"\"\"\n",
    "    Epoch 1/10\n",
    "    2022-04-20 11:18:40.482091: I tensorflow/stream_executor/cuda/cuda_dnn.cc:366] Loaded cuDNN version 8200\n",
    "    2022-04-20 11:18:41.662304: W tensorflow/stream_executor/gpu/asm_compiler.cc:111] *** WARNING *** You are using ptxas 11.0.221, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
    "\n",
    "    You may not need to update to CUDA 11.1; cherry-picking the ptxas binary is often sufficient.\n",
    "    2022-04-20 11:18:42.889289: I tensorflow/stream_executor/cuda/cuda_blas.cc:1774] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
    "    2000/2000 [==============================] - 32s 13ms/step - loss: 0.1851 - acc: 0.7246 - val_loss: 0.1669 - val_acc: 0.7571\n",
    "    Epoch 2/10\n",
    "    2000/2000 [==============================] - 25s 12ms/step - loss: 0.1605 - acc: 0.7685 - val_loss: 0.1575 - val_acc: 0.7748\n",
    "    Epoch 3/10\n",
    "    2000/2000 [==============================] - 25s 12ms/step - loss: 0.1557 - acc: 0.7765 - val_loss: 0.1555 - val_acc: 0.7767\n",
    "    Epoch 4/10\n",
    "    2000/2000 [==============================] - 25s 12ms/step - loss: 0.1545 - acc: 0.7779 - val_loss: 0.1545 - val_acc: 0.7784\n",
    "    Epoch 5/10\n",
    "    2000/2000 [==============================] - 25s 12ms/step - loss: 0.1535 - acc: 0.7797 - val_loss: 0.1537 - val_acc: 0.7798\n",
    "    Epoch 6/10\n",
    "    2000/2000 [==============================] - 25s 12ms/step - loss: 0.1529 - acc: 0.7808 - val_loss: 0.1531 - val_acc: 0.7807\n",
    "    Epoch 7/10\n",
    "    2000/2000 [==============================] - 25s 12ms/step - loss: 0.1525 - acc: 0.7813 - val_loss: 0.1527 - val_acc: 0.7813\n",
    "    Epoch 8/10\n",
    "    2000/2000 [==============================] - 25s 12ms/step - loss: 0.1522 - acc: 0.7818 - val_loss: 0.1526 - val_acc: 0.7813\n",
    "    Epoch 9/10\n",
    "    2000/2000 [==============================] - 25s 12ms/step - loss: 0.1519 - acc: 0.7823 - val_loss: 0.1522 - val_acc: 0.7821\n",
    "    Epoch 10/10\n",
    "    2000/2000 [==============================] - 25s 12ms/step - loss: 0.1517 - acc: 0.7827 - val_loss: 0.1519 - val_acc: 0.7825\n",
    "    Best validation accuracy:  0.7825149893760681\n",
    "    \"\"\"\n",
    "    #--- prepare GPU\n",
    "    import tensorflow as tf\n",
    "    gpus = tf.config.experimental.list_physical_devices(\"GPU\")\n",
    "    tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
    "    if set_memory_growth:\n",
    "        tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "\n",
    "    from keras.models import Model\n",
    "    from keras.layers import Dense, Conv1D, Conv2D, Input, Reshape, Permute, Add, Flatten, BatchNormalization, Activation\n",
    "    from keras.layers import Concatenate, MaxPooling2D\n",
    "    from keras.regularizers import l2\n",
    "    import numpy as np\n",
    "    \n",
    "    img_sqrt = int(np.sqrt(input_neurons))\n",
    "    \n",
    "    #num_blocks=2 \n",
    "    num_filters=32*4\n",
    "    d1=64*model_strength\n",
    "    d2=64*model_strength\n",
    "    #word_size=16\n",
    "    ks=3\n",
    "    depth=model_strength\n",
    "    reg_param=10**-5       \n",
    "    final_activation='sigmoid'\n",
    "    \n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.002, amsgrad=True) #'Adam'\n",
    "    loss = 'mse' #'Huber'\n",
    "    \n",
    "    #loss = 'mse'\n",
    "\n",
    "    # put input in square shape instead of word-like structure\n",
    "    inp = Input(shape=(input_neurons,));\n",
    "    #rs = Reshape((2 * num_blocks, word_size))(inp);\n",
    "    rs = Reshape((img_sqrt, img_sqrt))(inp) # changed rs = Reshape((img_sqrt, img_sqrt, 1))(inp)\n",
    "    \n",
    "    #---- search correlations in one direction\n",
    "    conv0 = Conv1D(num_filters, kernel_size=1, padding='same', kernel_regularizer=l2(reg_param))(rs); # changed\n",
    "    conv0 = BatchNormalization()(conv0);\n",
    "    conv0 = Activation('relu')(conv0);\n",
    "    #add residual blocks \n",
    "    shortcut = conv0;\n",
    "    for i in range(depth):\n",
    "        conv1 = Conv1D(num_filters, kernel_size=ks, padding='same', kernel_regularizer=l2(reg_param))(shortcut);\n",
    "        conv1 = BatchNormalization()(conv1);\n",
    "        conv1 = Activation('relu')(conv1);\n",
    "        conv2 = Conv1D(num_filters, kernel_size=ks, padding='same',kernel_regularizer=l2(reg_param))(conv1);\n",
    "        conv2 = BatchNormalization()(conv2);\n",
    "        conv2 = Activation('relu')(conv2);\n",
    "        shortcut = Add()([shortcut, conv2]);\n",
    "        \n",
    "    #---- search correlations in the other direction\n",
    "    perm = Permute((2,1))(rs); \n",
    "    conv0 = Conv1D(num_filters, kernel_size=1, padding='same', kernel_regularizer=l2(reg_param))(perm); # changed\n",
    "    conv0 = BatchNormalization()(conv0);\n",
    "    conv0 = Activation('relu')(conv0);\n",
    "    shortcut1 = conv0;\n",
    "    for i in range(depth):\n",
    "        conv1 = Conv1D(num_filters, kernel_size=ks, padding='same', kernel_regularizer=l2(reg_param))(shortcut1);\n",
    "        conv1 = BatchNormalization()(conv1);\n",
    "        conv1 = Activation('relu')(conv1);\n",
    "        conv2 = Conv1D(num_filters, kernel_size=ks, padding='same',kernel_regularizer=l2(reg_param))(conv1);\n",
    "        conv2 = BatchNormalization()(conv2);\n",
    "        conv2 = Activation('relu')(conv2);\n",
    "        shortcut1 = Add()([shortcut1, conv2]);\n",
    "\n",
    "    #---- bring them together\n",
    "    shortcut = Concatenate()([shortcut, shortcut1])#Add()([shortcut, shortcut1])\n",
    "    #add prediction head\n",
    "    flat1 = Flatten()(shortcut);\n",
    "    dense1 = Dense(d1,kernel_regularizer=l2(reg_param))(flat1);\n",
    "    dense1 = BatchNormalization()(dense1);\n",
    "    dense1 = Activation('relu')(dense1);\n",
    "    dense2 = Dense(d2, kernel_regularizer=l2(reg_param))(dense1);\n",
    "    dense2 = BatchNormalization()(dense2);\n",
    "    dense2 = Activation('relu')(dense2);\n",
    "    out = Dense(output_neurons, activation=final_activation, kernel_regularizer=l2(reg_param))(dense2);\n",
    "    model = Model(inputs=inp, outputs=out);\n",
    "    #-------------#-------------#-------------#-------------#\n",
    "    \n",
    "    model.compile(loss=loss,\n",
    "                  optimizer=optimizer,\n",
    "                  run_eagerly=False, metrics=['acc'])  \n",
    "        \n",
    "#     from gohr import cyclic_lr\n",
    "#     from keras.callbacks import LearningRateScheduler\n",
    "#     lr = LearningRateScheduler(cyclic_lr(10,0.002, 0.0001));\n",
    "    \n",
    "    model.callbacks = []# [lr]\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e177bf5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-20T12:02:19.839825Z",
     "start_time": "2022-04-20T12:02:19.826172Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def create_gohr_generalized_model(input_neurons=32, output_neurons=10, model_strength=1, set_memory_growth=True):\n",
    "    \"\"\"\n",
    "    CHANGE ME\n",
    "    \"\"\"\n",
    "    #--- prepare GPU\n",
    "    import tensorflow as tf\n",
    "    gpus = tf.config.experimental.list_physical_devices(\"GPU\")\n",
    "    tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
    "    if set_memory_growth:\n",
    "        tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "\n",
    "    from keras.models import Model\n",
    "    from keras.layers import Dense, Conv1D, Conv2D, Input, Reshape, Permute, Add, Flatten, BatchNormalization, Activation\n",
    "    from keras.layers import Concatenate, MaxPooling2D\n",
    "    from keras.regularizers import l2\n",
    "    import numpy as np\n",
    "    \n",
    "    img_sqrt = int(np.sqrt(input_neurons))\n",
    "    \n",
    "    #num_blocks=2 \n",
    "    num_filters=32*4\n",
    "    d1=64*model_strength\n",
    "    d2=64*model_strength\n",
    "    #word_size=16\n",
    "    ks=3\n",
    "    depth=model_strength\n",
    "    reg_param=10**-5       \n",
    "    final_activation='sigmoid'\n",
    "    \n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.002, amsgrad=True) #'Adam'\n",
    "    loss = 'mse' #'Huber'\n",
    "    \n",
    "    #loss = 'mse'\n",
    "\n",
    "    # put input in square shape instead of word-like structure\n",
    "    inp = Input(shape=(input_neurons,));\n",
    "    #rs = Reshape((2 * num_blocks, word_size))(inp);\n",
    "    rs = Reshape((img_sqrt, img_sqrt))(inp) # changed rs = Reshape((img_sqrt, img_sqrt, 1))(inp)\n",
    "    rs = Permute((2,1))(rs)\n",
    "    #---- search correlations in one direction\n",
    "    conv0 = Conv1D(num_filters, kernel_size=1, padding='same', kernel_regularizer=l2(reg_param))(rs); # changed\n",
    "    conv0 = BatchNormalization()(conv0);\n",
    "    conv0 = Activation('relu')(conv0);\n",
    "    #add residual blocks \n",
    "    shortcut = conv0;\n",
    "    for i in range(depth):\n",
    "        conv1 = Conv1D(num_filters, kernel_size=ks, padding='same', kernel_regularizer=l2(reg_param))(shortcut);\n",
    "        conv1 = BatchNormalization()(conv1);\n",
    "        conv1 = Activation('relu')(conv1);\n",
    "        conv2 = Conv1D(num_filters, kernel_size=ks, padding='same',kernel_regularizer=l2(reg_param))(conv1);\n",
    "        conv2 = BatchNormalization()(conv2);\n",
    "        conv2 = Activation('relu')(conv2);\n",
    "        shortcut = Add()([shortcut, conv2]);\n",
    "        \n",
    "    #---- search correlations in the other direction\n",
    "#     perm = Permute((2,1))(rs); \n",
    "#     conv0 = Conv1D(num_filters, kernel_size=1, padding='same', kernel_regularizer=l2(reg_param))(perm); # changed\n",
    "#     conv0 = BatchNormalization()(conv0);\n",
    "#     conv0 = Activation('relu')(conv0);\n",
    "#     shortcut1 = conv0;\n",
    "#     for i in range(depth):\n",
    "#         conv1 = Conv1D(num_filters, kernel_size=ks, padding='same', kernel_regularizer=l2(reg_param))(shortcut1);\n",
    "#         conv1 = BatchNormalization()(conv1);\n",
    "#         conv1 = Activation('relu')(conv1);\n",
    "#         conv2 = Conv1D(num_filters, kernel_size=ks, padding='same',kernel_regularizer=l2(reg_param))(conv1);\n",
    "#         conv2 = BatchNormalization()(conv2);\n",
    "#         conv2 = Activation('relu')(conv2);\n",
    "#         shortcut1 = Add()([shortcut1, conv2]);\n",
    "\n",
    "    #---- bring them together\n",
    "    #shortcut = Concatenate()([shortcut, shortcut1])#Add()([shortcut, shortcut1])\n",
    "    #add prediction head\n",
    "    flat1 = Flatten()(shortcut);\n",
    "    dense1 = Dense(d1,kernel_regularizer=l2(reg_param))(flat1);\n",
    "    dense1 = BatchNormalization()(dense1);\n",
    "    dense1 = Activation('relu')(dense1);\n",
    "    dense2 = Dense(d2, kernel_regularizer=l2(reg_param))(dense1);\n",
    "    dense2 = BatchNormalization()(dense2);\n",
    "    dense2 = Activation('relu')(dense2);\n",
    "    out = Dense(output_neurons, activation=final_activation, kernel_regularizer=l2(reg_param))(dense2);\n",
    "    model = Model(inputs=inp, outputs=out);\n",
    "    #-------------#-------------#-------------#-------------#\n",
    "    \n",
    "    model.compile(loss=loss,\n",
    "                  optimizer=optimizer,\n",
    "                  run_eagerly=False, metrics=['acc'])  \n",
    "        \n",
    "#     from gohr import cyclic_lr\n",
    "#     from keras.callbacks import LearningRateScheduler\n",
    "#     lr = LearningRateScheduler(cyclic_lr(10,0.002, 0.0001));\n",
    "    \n",
    "    model.callbacks = []# [lr]\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb01134",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-20T10:28:31.101301Z",
     "start_time": "2022-04-20T10:28:31.089242Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90fdfd73",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-20T12:02:21.019429Z",
     "start_time": "2022-04-20T12:02:21.015757Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def train_speck_distinguisher(num_epochs, nr, n, depth=1):\n",
    "    \n",
    "    from gohr import bs\n",
    "\n",
    "    #create the network\n",
    "    net = create_gohr_generalized_model(input_neurons=64, output_neurons=1, model_strength=1);\n",
    "    \n",
    "    import numpy as np\n",
    "    #net.compile(optimizer='adam',loss='mse',metrics=['acc']);\n",
    "    #generate training and validation data\n",
    "    X = np.load('data_train_full.npy')\n",
    "    Y = np.load('data_train_labels.npy')\n",
    "    X_eval = np.load('data_test_full.npy')\n",
    "    Y_eval = np.load('data_test_labels.npy')\n",
    "    #create learnrate schedule\n",
    "    #train and evaluate\n",
    "    h = net.fit(X,Y,epochs=num_epochs,batch_size=bs,validation_data=(X_eval, Y_eval));\n",
    "\n",
    "    print(\"Best validation accuracy: \", np.max(h.history['val_acc']));\n",
    "\n",
    "    return(X, Y);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7dfde565",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-20T12:03:28.608786Z",
     "start_time": "2022-04-20T12:02:21.268297Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 16s 7ms/step - loss: 0.1867 - acc: 0.7205 - val_loss: 0.1672 - val_acc: 0.7569\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 13s 7ms/step - loss: 0.1627 - acc: 0.7634 - val_loss: 0.1612 - val_acc: 0.7664\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 0.1583 - acc: 0.7712 - val_loss: 0.1567 - val_acc: 0.7746\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 0.1552 - acc: 0.7768 - val_loss: 0.1550 - val_acc: 0.7772\n",
      "Epoch 5/10\n",
      "1524/2000 [=====================>........] - ETA: 2s - loss: 0.1543 - acc: 0.7778"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_3106337/3516618139.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0;31m# with kernel regularizer  77.4% val_acc\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[0;31m#    (without kernel regularizer: 77.0% val_acc)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 3\u001B[0;31m \u001B[0mtrain_speck_distinguisher\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m10\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m6\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m10\u001B[0m\u001B[0;34m**\u001B[0m\u001B[0;36m7\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m/tmp/ipykernel_3106337/2167173066.py\u001B[0m in \u001B[0;36mtrain_speck_distinguisher\u001B[0;34m(num_epochs, nr, n, depth)\u001B[0m\n\u001B[1;32m     15\u001B[0m     \u001B[0;31m#create learnrate schedule\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     16\u001B[0m     \u001B[0;31m#train and evaluate\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 17\u001B[0;31m     \u001B[0mh\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mnet\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mX\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mY\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mepochs\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mnum_epochs\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mbatch_size\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mbs\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mvalidation_data\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mX_eval\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mY_eval\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m;\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     18\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     19\u001B[0m     \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"Best validation accuracy: \"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmax\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mh\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mhistory\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'val_acc'\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m;\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/miniconda3/envs/E2202/lib/python3.8/site-packages/keras/utils/traceback_utils.py\u001B[0m in \u001B[0;36merror_handler\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     62\u001B[0m     \u001B[0mfiltered_tb\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     63\u001B[0m     \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 64\u001B[0;31m       \u001B[0;32mreturn\u001B[0m \u001B[0mfn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     65\u001B[0m     \u001B[0;32mexcept\u001B[0m \u001B[0mException\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m:\u001B[0m  \u001B[0;31m# pylint: disable=broad-except\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     66\u001B[0m       \u001B[0mfiltered_tb\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0m_process_traceback_frames\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0me\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__traceback__\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/miniconda3/envs/E2202/lib/python3.8/site-packages/keras/engine/training.py\u001B[0m in \u001B[0;36mfit\u001B[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001B[0m\n\u001B[1;32m   1214\u001B[0m                 _r=1):\n\u001B[1;32m   1215\u001B[0m               \u001B[0mcallbacks\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mon_train_batch_begin\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mstep\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1216\u001B[0;31m               \u001B[0mtmp_logs\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtrain_function\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0miterator\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1217\u001B[0m               \u001B[0;32mif\u001B[0m \u001B[0mdata_handler\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshould_sync\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1218\u001B[0m                 \u001B[0mcontext\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0masync_wait\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/miniconda3/envs/E2202/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py\u001B[0m in \u001B[0;36merror_handler\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    148\u001B[0m     \u001B[0mfiltered_tb\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    149\u001B[0m     \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 150\u001B[0;31m       \u001B[0;32mreturn\u001B[0m \u001B[0mfn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    151\u001B[0m     \u001B[0;32mexcept\u001B[0m \u001B[0mException\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    152\u001B[0m       \u001B[0mfiltered_tb\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0m_process_traceback_frames\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0me\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__traceback__\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/miniconda3/envs/E2202/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args, **kwds)\u001B[0m\n\u001B[1;32m    908\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    909\u001B[0m       \u001B[0;32mwith\u001B[0m \u001B[0mOptionalXlaContext\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jit_compile\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 910\u001B[0;31m         \u001B[0mresult\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_call\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwds\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    911\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    912\u001B[0m       \u001B[0mnew_tracing_count\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mexperimental_get_tracing_count\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/miniconda3/envs/E2202/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001B[0m in \u001B[0;36m_call\u001B[0;34m(self, *args, **kwds)\u001B[0m\n\u001B[1;32m    940\u001B[0m       \u001B[0;31m# In this case we have created variables on the first call, so we run the\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    941\u001B[0m       \u001B[0;31m# defunned version which is guaranteed to never create variables.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 942\u001B[0;31m       \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_stateless_fn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwds\u001B[0m\u001B[0;34m)\u001B[0m  \u001B[0;31m# pylint: disable=not-callable\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    943\u001B[0m     \u001B[0;32melif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_stateful_fn\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    944\u001B[0m       \u001B[0;31m# Release the lock early so that multiple threads can perform the call\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/miniconda3/envs/E2202/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   3128\u001B[0m       (graph_function,\n\u001B[1;32m   3129\u001B[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001B[0;32m-> 3130\u001B[0;31m     return graph_function._call_flat(\n\u001B[0m\u001B[1;32m   3131\u001B[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001B[1;32m   3132\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/miniconda3/envs/E2202/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001B[0m in \u001B[0;36m_call_flat\u001B[0;34m(self, args, captured_inputs, cancellation_manager)\u001B[0m\n\u001B[1;32m   1957\u001B[0m         and executing_eagerly):\n\u001B[1;32m   1958\u001B[0m       \u001B[0;31m# No tape is watching; skip to running the function.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1959\u001B[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001B[0m\u001B[1;32m   1960\u001B[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001B[1;32m   1961\u001B[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001B[0;32m~/miniconda3/envs/E2202/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001B[0m in \u001B[0;36mcall\u001B[0;34m(self, ctx, args, cancellation_manager)\u001B[0m\n\u001B[1;32m    596\u001B[0m       \u001B[0;32mwith\u001B[0m \u001B[0m_InterpolateFunctionError\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    597\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mcancellation_manager\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 598\u001B[0;31m           outputs = execute.execute(\n\u001B[0m\u001B[1;32m    599\u001B[0m               \u001B[0mstr\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msignature\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mname\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    600\u001B[0m               \u001B[0mnum_outputs\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_num_outputs\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/miniconda3/envs/E2202/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\u001B[0m in \u001B[0;36mquick_execute\u001B[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001B[0m\n\u001B[1;32m     56\u001B[0m   \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     57\u001B[0m     \u001B[0mctx\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mensure_initialized\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 58\u001B[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001B[0m\u001B[1;32m     59\u001B[0m                                         inputs, attrs, num_outputs)\n\u001B[1;32m     60\u001B[0m   \u001B[0;32mexcept\u001B[0m \u001B[0mcore\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_NotOkStatusException\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# with kernel regularizer  77.4% val_acc\n",
    "#    (without kernel regularizer: 77.0% val_acc)\n",
    "train_speck_distinguisher(10, 6, 10**7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe609c7",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}