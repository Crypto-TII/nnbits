{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79d7c480",
   "metadata": {
    "colab": {
     "accelerator": "GPU"
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "This demo shows how to use the `nnbits` repository by \n",
    "\n",
    "1. generating the avalanche dataset for round 7 of Speck 32/64\n",
    "2. exploring the computational resources\n",
    "3. running an ensemble neural network analysis\n",
    "\n",
    "It reproduces the corresponding results from the manuscript."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d3ffec",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Install-Requirements\" data-toc-modified-id=\"Install-Requirements-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Install Requirements</a></span></li><li><span><a href=\"#Choose-a-round-to-analyze\" data-toc-modified-id=\"Choose-a-round-to-analyze-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Choose a round to analyze</a></span></li><li><span><a href=\"#Generate-avalanche-data\" data-toc-modified-id=\"Generate-avalanche-data-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Generate avalanche data</a></span></li><li><span><a href=\"#Have-a-look-at-the-available-resources\" data-toc-modified-id=\"Have-a-look-at-the-available-resources-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Have a look at the available resources</a></span></li><li><span><a href=\"#Run-nnbits-neural-network-analysis\" data-toc-modified-id=\"Run-nnbits-neural-network-analysis-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Run <code>nnbits</code> neural network analysis</a></span></li><li><span><a href=\"#Analyze-the-results\" data-toc-modified-id=\"Analyze-the-results-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Analyze the results</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# How to use this demo?\n",
    "Please execute the following cells by pressing \n",
    "\n",
    "`Shift + Enter`.\n",
    "\n",
    "<font color='blue'>**Important**</font> If you are executing the Notebook on <font color='blue'>Google Colaboratory, please enable GPU support by clicking on `Runtime > Change Runtime Type > Hardware accelerator \"GPU\" > Save`</font>:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Install Requirements "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Clone the GitHub repository:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!git clone https://github.com/Crypto-TII/nnbits"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Change directory into the cloned repository:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%cd nnbits/"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The following modules are required: "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%cat requirements.txt"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Install the requirements by running the following cell. "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Choose a round to analyze "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Here we will analyze round 7 of Speck 32/64.\n",
    "# The corresponding round-id is round 6 (we start counting the round-id from zero):\n",
    "ROUND_IDs = [6]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Generate avalanche data "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Create a folder for the data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!mkdir 'speck_32_64'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Generate `number_of_samples = 300_000` avalanche units (this takes about one minute in Google Colab):"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#### CONFIGURATION ######\n",
    "\n",
    "number_of_samples = 300_000\n",
    "\n",
    "#########################\n",
    "\n",
    "from nnbits.avalanche_data_generator.speck_32_64 import speck_k64_p32_o32_r22 as data_generator\n",
    "import numpy as np\n",
    "\n",
    "dataset = data_generator.generate_avalanche_dataset(int(number_of_samples))\n",
    "\n",
    "for r in ROUND_IDs:\n",
    "    np.save(f\"speck_32_64/round{r}_sequences300k.npy\", dataset[r])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Have a look at the available resources \n",
    "\n",
    "The Nvidia A100-DGX server provides \n",
    "\n",
    "* `4 A100-SXM GPUs, each with 40'536 MiB memory (and in addition a graphical GPU)` \n",
    "* `128 CPU cores`\n",
    "\n",
    "In comparison, the free Google Colaboratory provides\n",
    "\n",
    "* `1 Tesla T4 GPU with 15'109 MiB memory`\n",
    "* `2 CPU cores`\n",
    "\n",
    "The available resources can be explored by using for example the `ray.available_resources()` or `nvidia-smi`:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import ray\n",
    "ray.init()\n",
    "ray.available_resources()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Run `nnbits` neural network analysis "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "If you are executing the Notebook on <font color='blue'>Google Colaboratory, please ensure that the following four lines are identical in the cell below</font>:\n",
    "\n",
    "```python\n",
    "...\n",
    "# hardware settings\n",
    "'N_GPUS': 1, \n",
    "'N_ACTORS_PER_GPU': 2,\n",
    "'GPU_PER_ACTOR': 0.5,\n",
    "'CPU_PER_ACTOR': 0.5,\n",
    "...\n",
    "```\n",
    "\n",
    "If you work on a 4-GPU server with 40 GB memory per GPU, the following settings would be appropriate \n",
    "```python\n",
    "...\n",
    "# hardware settings\n",
    "'N_GPUS': 4,\n",
    "'N_ACTORS_PER_GPU': 6,\n",
    "'GPU_PER_ACTOR': 0.15,\n",
    "'CPU_PER_ACTOR': 5, \n",
    "...\n",
    "```"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import toml\n",
    "from filemanager import FileManager\n",
    "\n",
    "for r in ROUND_IDs:\n",
    "    \n",
    "    # choose where to save the data\n",
    "    savepath = f'demo_speck32_roundid{r}'\n",
    "    !rm -r '{savepath}'\n",
    "    !mkdir '{savepath}'\n",
    "    \n",
    "    # choose the settings\n",
    "    config = {'DATAPATH': f'speck_32_64/round{r}_sequences300k.npy',\n",
    "               # ensemble settings\n",
    "                 'NEURAL_NETWORK_MODEL': 'gohr_generalized', \n",
    "                 'NEURAL_NETWORKS': 100, \n",
    "                 'SELECT_BITS_STRATEGY': 'random', \n",
    "                 'INPUT_DATA_OP': 'zero', \n",
    "                 'N_RANDOM_BITS': 63,\n",
    "               # hardware settings\n",
    "                'N_GPUS': 1, \n",
    "                'N_ACTORS_PER_GPU': 2,\n",
    "                'GPU_PER_ACTOR': 0.5,\n",
    "                'CPU_PER_ACTOR': 0.5,\n",
    "               # training settings\n",
    "                 'N_EPOCHS': 5,\n",
    "                 'N_TRAIN': 145_000,\n",
    "                 'N_VAL': 145_000,\n",
    "                 'N_TEST': 0,\n",
    "                 'BATCHSIZE': 5000}\n",
    "    \n",
    "    # save the configuration file\n",
    "    F = FileManager(savepath)\n",
    "\n",
    "    with open(F.filename_config(), 'w') as configfile:\n",
    "        toml.dump(config, configfile)\n",
    "        \n",
    "    #============ run the analysis ============\n",
    "    !python -m nnbits.run --savepath '{savepath}'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Analyze the results "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Each neural network has been tested on the test-data and the results are saved in the `test_accuracies` folder:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!ls 'demo_speck32_roundid6'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!ls 'demo_speck32_roundid6/test_accuracies'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# the file-manager is a simple module which knows under which path to search for e.g. the test_accuracies:\n",
    "from nnbits.filemanager import FileManager\n",
    "F = FileManager('demo_speck32_roundid6') "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# given a file-manager, the get_X function loads all the test_accuracies from above into a single array:\n",
    "from nnbits.bitanalysis import get_X\n",
    "X = get_X(F)\n",
    "# X contains one row for each of the `N` neural network and one column for each of the `n` bits:\n",
    "X.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# the bit accuracies are calculated by taking the mean over all neural networks which predict the particular bit \n",
    "# (axis=0):\n",
    "# (the following three lines catch a runtime warning if a mean-calculation is empty. This happens if not all bits have been analyzed yet.)\n",
    "import warnings\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\", category=RuntimeWarning)\n",
    "    bit_accuracies = np.nanmean(X, axis=0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame({'bit': np.arange(1024), \n",
    "                   'mean accuracy': bit_accuracies})\n",
    "\n",
    "df.sort_values('mean accuracy', ascending=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# here we visualize the obtained `bit_accuracies`:\n",
    "plt.figure(figsize=(10, 3), dpi=150)\n",
    "plt.plot(bit_accuracies, 'o', markersize=3, linestyle = 'None', label=\"mean validation accuracy of each bit\")\n",
    "# find bit with maximum accuracy\n",
    "dfm = df[df['mean accuracy'] == max(df['mean accuracy'])]\n",
    "plt.plot(dfm['bit'], dfm['mean accuracy'], marker='x', c='C0', linestyle = 'None', label=f\"best bit {int(dfm['bit'])} with {float(dfm['mean accuracy'])*100:.1f}% accuracy\")\n",
    "\n",
    "plt.legend(loc='upper left')\n",
    "plt.xlabel('bit id')\n",
    "plt.ylabel('validation accuracy')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb3da1d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}